{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 1 - NLP\n",
    "\n",
    "Neste exercício faremos todos os pré-processamentos necessários para que sequências de textos possam ser interpretadas por Redes Neurais.\n",
    "\n",
    "A tarefa que exploraremos é a Classificaćão de Sentimento usando um dataset de revisões de restaurantes (YELP), produtos (Amazon) e filmes (IMDB) [link](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).\n",
    "\n",
    "A nossa tarefa consiste em analisar a revisão e classificá-la entre \"positiva\" ou \"negativa\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos explorar o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__ #Talvez seja necessário instalar tensorflow 2 antes de iniciar esse notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso dataset tem 3 colunas:\n",
    "\n",
    "- sentence: O texto da revisão\n",
    "- label: 1 para texto positivo e 0 para negativo\n",
    "- source: yelp, amazon ou imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath_dict = {'yelp':   'data/sentiment/yelp_labelled.txt',\n",
    "                 'amazon': 'data/sentiment/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'data/sentiment/imdb_labelled.txt'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos separar nosso dataset de modo que 15% dele seja reservado para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train = 0.85\n",
    "len_train = int(len(df)*perc_train)\n",
    "\n",
    "dataset_train = df.iloc[0:len_train, :-1]\n",
    "dataset_test = df.iloc[len_train:, :-1]\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos nosso dataset organizado, o primeiro passo é processar o texto para que seja legível por uma Rede Neural\n",
    "\n",
    "O primeiro passo é gerar o vocabulário a partir da base de treinamento com a classe [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "Essa classe executa diversas rotinas de pré-processamento úteis, entre elas:\n",
    "\n",
    "- Remover pontuacões.\n",
    "- através do parâmetro `num_words`, permite limitar o tamanho do vocabulário, descartando palavras incomuns.\n",
    "- Normaliza capitalizacao com `lower=True`\n",
    "\n",
    "Porém, antes de utilizar a classe, vamos remover stopwords do texto.\n",
    "\n",
    "Stopwords são palavras com serventia apenas sintática, isso é, são irrelevantes para classificar o \"sentimento\" da sentenca (leia mais sobre stopwords [aqui](https://demacdolincoln.github.io/anotacoes-nlp/posts/pre-processamento-de-textos/#id2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estamos adicionando stopwords manualmente aqui. Também é possível baixá-las do módulo nltk\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "\n",
    "#Adicione seu código para Excluir todas as stopwords de todos os exemplos de treinamento\n",
    "dataset_train.loc[:,'sentence'] = dataset_train.loc[:,'sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in stopwords]))\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos gerar o vocabulário e codificar as sentencas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab_size = 500   #Tamanho máximo do vocabulário\n",
    "oov_token = '<OOV>'   # Token usado caso alguma palavra não for encontrada no vocabulário\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, lower=True, oov_token = oov_token)\n",
    "tokenizer.fit_on_texts(dataset_train.loc[:, 'sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do atributo `word_index`, podemos consultar o vocabulário gerado. As primeiras palavras são as mais comuns.\n",
    "\n",
    "Em seguida, codificamos o dataset de treinamento e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_sequences = tokenizer.texts_to_sequences(dataset_train.loc[:,'sentence'])\n",
    "dataset_test_sequences = tokenizer.texts_to_sequences(dataset_test.loc[:,'sentence'])\n",
    "print(dataset_train_sequences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O último passo de pré-processamento agora consiste em realizar o padding das sequências.\n",
    "\n",
    "Para isso, utilizaremos a funcão [`pad_sequences`](https://keras.io/preprocessing/sequence/)\n",
    "\n",
    "Os principais argumentos dessa funcão são:\n",
    "\n",
    "- `maxlen`: tamanho da sequência a ser gerada.\n",
    "- `padding`: 'pre' para adicionar zeros à esquerda e 'post' para adicionar zeros à direita.\n",
    "- `truncating`: 'pre' para remover palavras no comeco da frase se for maior que o tamanho especificado, 'post' para remover do final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500  #Tamanho máximo da frase\n",
    "padding_type = 'post'\n",
    "truncating_type = 'post'\n",
    "\n",
    "dataset_train_sequences = pad_sequences(dataset_train_sequences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "dataset_test_sequences = pad_sequences(dataset_test_sequences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "\n",
    "print(len(dataset_train_sequences[0]))\n",
    "print(len(dataset_train_sequences[1]))\n",
    "print(dataset_train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que as sentencas estão em um formato favorável, podemos treinar nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicione a sua arquitetura, lembrando que a entrada tem tamanho maxlen e a saída 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential()\n",
    "#Sua arquitetura\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina aqui seu otimizador e sua loss\n",
    "optimizer = \n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "num_epochs = 20\n",
    "\n",
    "train_seqs = dataset_train_sequences\n",
    "train_labels = np.array(dataset_train.loc[:, 'label'])\n",
    "test_seqs = dataset_test_sequences\n",
    "test_labels = np.array(dataset_test.loc[:, 'label'])\n",
    "\n",
    "print(len(train_seqs))\n",
    "print(len(train_labels))\n",
    "print(len(test_seqs))\n",
    "print(len(test_labels))\n",
    "\n",
    "model.fit(train_seqs,train_labels, epochs = num_epochs, validation_data=(test_seqs,test_labels), verbose=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar se as classificacões fazem sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_test_sentences = ['very good movie', 'terrible taste', 'worst product ever']\n",
    "#codificando\n",
    "test_sentences = tokenizer.texts_to_sequences(or_test_sentences)\n",
    "test_sentences = pad_sequences(test_sentences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "\n",
    "print(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_sentences)\n",
    "print(or_test_sentences)\n",
    "print(predictions > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie como o número de dimensões do embedding, o tipo do padding, o tamanho do vocabulário, o tamanho máximo de sentenca, etc. contribuem para a qualidade do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
