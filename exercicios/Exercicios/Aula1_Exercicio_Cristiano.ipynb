{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 1 - NLP\n",
    "\n",
    "Neste exercício faremos todos os pré-processamentos necessários para que sequências de textos possam ser interpretadas por Redes Neurais.\n",
    "\n",
    "A tarefa que exploraremos é a Classificaćão de Sentimento usando um dataset de revisões de restaurantes (YELP), produtos (Amazon) e filmes (IMDB) [link](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).\n",
    "\n",
    "A nossa tarefa consiste em analisar a revisão e classificá-la entre \"positiva\" ou \"negativa\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos explorar o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__ #Talvez seja necessário instalar tensorflow 2 antes de iniciar esse notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso dataset tem 3 colunas:\n",
    "\n",
    "- sentence: O texto da revisão\n",
    "- label: 1 para texto positivo e 0 para negativo\n",
    "- source: yelp, amazon ou imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Same problem as others have mentioned.</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The directing seems too pretentious.</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ate there twice on my last visit, and especi...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pros:-Good camera - very nice pictures , also ...</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not even a \"hello, we will be right with you.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  source\n",
       "0             Same problem as others have mentioned.      0  amazon\n",
       "1             The directing seems too pretentious.        0    imdb\n",
       "2  I ate there twice on my last visit, and especi...      1    yelp\n",
       "3  Pros:-Good camera - very nice pictures , also ...      1  amazon\n",
       "4     not even a \"hello, we will be right with you.\"      0    yelp"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filepath_dict = {'yelp':   'data/sentiment/yelp_labelled.txt',\n",
    "                 'amazon': 'data/sentiment/amazon_cells_labelled.txt',\n",
    "                 'imdb':   'data/sentiment/imdb_labelled.txt'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos separar nosso dataset de modo que 15% dele seja reservado para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335\n",
      "413\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Same problem as others have mentioned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The directing seems too pretentious.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ate there twice on my last visit, and especi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pros:-Good camera - very nice pictures , also ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not even a \"hello, we will be right with you.\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0             Same problem as others have mentioned.      0\n",
       "1             The directing seems too pretentious.        0\n",
       "2  I ate there twice on my last visit, and especi...      1\n",
       "3  Pros:-Good camera - very nice pictures , also ...      1\n",
       "4     not even a \"hello, we will be right with you.\"      0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_train = 0.85\n",
    "len_train = int(len(df)*perc_train)\n",
    "\n",
    "dataset_train = df.iloc[0:len_train, :-1]\n",
    "dataset_test = df.iloc[len_train:, :-1]\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos nosso dataset organizado, o primeiro passo é processar o texto para que seja legível por uma Rede Neural\n",
    "\n",
    "O primeiro passo é gerar o vocabulário a partir da base de treinamento com a classe [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "Essa classe executa diversas rotinas de pré-processamento úteis, entre elas:\n",
    "\n",
    "- Remover pontuacões.\n",
    "- através do parâmetro `num_words`, permite limitar o tamanho do vocabulário, descartando palavras incomuns.\n",
    "- Normaliza capitalizacao com `lower=True`\n",
    "\n",
    "Porém, antes de utilizar a classe, vamos remover stopwords do texto.\n",
    "\n",
    "Stopwords são palavras com serventia apenas sintática, isso é, são irrelevantes para classificar o \"sentimento\" da sentenca (leia mais sobre stopwords [aqui](https://demacdolincoln.github.io/anotacoes-nlp/posts/pre-processamento-de-textos/#id2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Same problem others mentioned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The directing seems pretentious.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ate twice last visit, especially enjoyed sal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pros:-Good camera - nice pictures , also cool ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>even \"hello, right you.\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                     Same problem others mentioned.      0\n",
       "1                   The directing seems pretentious.      0\n",
       "2  I ate twice last visit, especially enjoyed sal...      1\n",
       "3  Pros:-Good camera - nice pictures , also cool ...      1\n",
       "4                           even \"hello, right you.\"      0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estamos adicionando stopwords manualmente aqui. Também é possível baixá-las do módulo nltk\n",
    "#stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "\n",
    "#Adicione seu código para Excluir todas as stopwords de todos os exemplos de treinamento\n",
    "dataset_train.loc[:,'sentence'] = dataset_train.loc[:,'sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in stopWords]))\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos gerar o vocabulário e codificar as sentencas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab_size = 500   #Tamanho máximo do vocabulário\n",
    "oov_token = '<OOV>'   # Token usado caso alguma palavra não for encontrada no vocabulário\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, lower=True, oov_token = oov_token)\n",
    "tokenizer.fit_on_texts(dataset_train.loc[:, 'sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do atributo `word_index`, podemos consultar o vocabulário gerado. As primeiras palavras são as mais comuns.\n",
    "\n",
    "Em seguida, codificamos o dataset de treinamento e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'i': 2,\n",
       " 'the': 3,\n",
       " 'it': 4,\n",
       " 'good': 5,\n",
       " 'great': 6,\n",
       " 'this': 7,\n",
       " 'movie': 8,\n",
       " 'phone': 9,\n",
       " 'film': 10,\n",
       " 'one': 11,\n",
       " '0': 12,\n",
       " 'food': 13,\n",
       " 'like': 14,\n",
       " 'place': 15,\n",
       " 'service': 16,\n",
       " '1': 17,\n",
       " 'time': 18,\n",
       " 'bad': 19,\n",
       " 'really': 20,\n",
       " 'well': 21,\n",
       " 'would': 22,\n",
       " 'even': 23,\n",
       " 'ever': 24,\n",
       " 'best': 25,\n",
       " 'quality': 26,\n",
       " 'back': 27,\n",
       " 'also': 28,\n",
       " 'go': 29,\n",
       " 'product': 30,\n",
       " 'love': 31,\n",
       " \"i've\": 32,\n",
       " 'work': 33,\n",
       " \"it's\": 34,\n",
       " 'get': 35,\n",
       " 'nice': 36,\n",
       " 'made': 37,\n",
       " 'not': 38,\n",
       " 'never': 39,\n",
       " 'recommend': 40,\n",
       " 'works': 41,\n",
       " \"i'm\": 42,\n",
       " 'very': 43,\n",
       " 'much': 44,\n",
       " 'first': 45,\n",
       " 'all': 46,\n",
       " 'sound': 47,\n",
       " 'excellent': 48,\n",
       " 'better': 49,\n",
       " 'battery': 50,\n",
       " 'way': 51,\n",
       " 'if': 52,\n",
       " 'pretty': 53,\n",
       " 'could': 54,\n",
       " 'headset': 55,\n",
       " 'still': 56,\n",
       " 'my': 57,\n",
       " 'you': 58,\n",
       " 'think': 59,\n",
       " 'use': 60,\n",
       " 'acting': 61,\n",
       " 'see': 62,\n",
       " 'and': 63,\n",
       " 'make': 64,\n",
       " 'got': 65,\n",
       " 'a': 66,\n",
       " 'but': 67,\n",
       " 'worst': 68,\n",
       " '2': 69,\n",
       " '10': 70,\n",
       " 'going': 71,\n",
       " 'enough': 72,\n",
       " 'we': 73,\n",
       " 'everything': 74,\n",
       " 'there': 75,\n",
       " 'disappointed': 76,\n",
       " 'say': 77,\n",
       " 'again': 78,\n",
       " 'thing': 79,\n",
       " 'little': 80,\n",
       " 'ear': 81,\n",
       " 'terrible': 82,\n",
       " 'experience': 83,\n",
       " 'so': 84,\n",
       " 'case': 85,\n",
       " 'real': 86,\n",
       " 'two': 87,\n",
       " 'right': 88,\n",
       " 'definitely': 89,\n",
       " 'came': 90,\n",
       " 'people': 91,\n",
       " 'price': 92,\n",
       " 'minutes': 93,\n",
       " 'look': 94,\n",
       " 'waste': 95,\n",
       " 'new': 96,\n",
       " 'amazing': 97,\n",
       " 'movies': 98,\n",
       " 'every': 99,\n",
       " 'that': 100,\n",
       " 'characters': 101,\n",
       " 'poor': 102,\n",
       " 'friendly': 103,\n",
       " 'plot': 104,\n",
       " 'money': 105,\n",
       " 'story': 106,\n",
       " 'used': 107,\n",
       " 'highly': 108,\n",
       " 'many': 109,\n",
       " 'here': 110,\n",
       " 'piece': 111,\n",
       " 'wonderful': 112,\n",
       " 'quite': 113,\n",
       " 'they': 114,\n",
       " 'long': 115,\n",
       " 'seen': 116,\n",
       " 'me': 117,\n",
       " 'nothing': 118,\n",
       " 'script': 119,\n",
       " 'want': 120,\n",
       " 'camera': 121,\n",
       " 'for': 122,\n",
       " 'what': 123,\n",
       " 'years': 124,\n",
       " 'films': 125,\n",
       " 'times': 126,\n",
       " \"can't\": 127,\n",
       " 'probably': 128,\n",
       " 'around': 129,\n",
       " 'car': 130,\n",
       " 'always': 131,\n",
       " 'vegas': 132,\n",
       " 'know': 133,\n",
       " 'life': 134,\n",
       " 'give': 135,\n",
       " 'buy': 136,\n",
       " 'found': 137,\n",
       " 'happy': 138,\n",
       " 'screen': 139,\n",
       " 'worth': 140,\n",
       " 'us': 141,\n",
       " 'worked': 142,\n",
       " 'funny': 143,\n",
       " 'too': 144,\n",
       " 'delicious': 145,\n",
       " 'however': 146,\n",
       " 'stars': 147,\n",
       " 'far': 148,\n",
       " 'in': 149,\n",
       " 'awful': 150,\n",
       " 'eat': 151,\n",
       " 'fine': 152,\n",
       " 'find': 153,\n",
       " 'charger': 154,\n",
       " 'is': 155,\n",
       " 'music': 156,\n",
       " \"don't\": 157,\n",
       " 'bought': 158,\n",
       " 'lot': 159,\n",
       " 'impressed': 160,\n",
       " 'no': 161,\n",
       " 'easy': 162,\n",
       " 'absolutely': 163,\n",
       " 'went': 164,\n",
       " 'cool': 165,\n",
       " 'awesome': 166,\n",
       " 'show': 167,\n",
       " 'anyone': 168,\n",
       " 'take': 169,\n",
       " 'thought': 170,\n",
       " 'watching': 171,\n",
       " 'comfortable': 172,\n",
       " '5': 173,\n",
       " 'job': 174,\n",
       " 'character': 175,\n",
       " 'ordered': 176,\n",
       " 'since': 177,\n",
       " 'overall': 178,\n",
       " 'end': 179,\n",
       " 'up': 180,\n",
       " 'another': 181,\n",
       " 'must': 182,\n",
       " \"i'd\": 183,\n",
       " 'horrible': 184,\n",
       " 'reception': 185,\n",
       " 'talk': 186,\n",
       " 'slow': 187,\n",
       " 'said': 188,\n",
       " 'night': 189,\n",
       " 'to': 190,\n",
       " 'day': 191,\n",
       " 'old': 192,\n",
       " 'watch': 193,\n",
       " 'big': 194,\n",
       " 'fantastic': 195,\n",
       " 'after': 196,\n",
       " 'cheap': 197,\n",
       " 'restaurant': 198,\n",
       " 'scenes': 199,\n",
       " 'last': 200,\n",
       " 'salad': 201,\n",
       " 'part': 202,\n",
       " 'next': 203,\n",
       " 'done': 204,\n",
       " 'loved': 205,\n",
       " 'try': 206,\n",
       " 'performance': 207,\n",
       " 'though': 208,\n",
       " 'pizza': 209,\n",
       " 'problem': 210,\n",
       " 'black': 211,\n",
       " 'fresh': 212,\n",
       " 'feel': 213,\n",
       " 'cast': 214,\n",
       " 'out': 215,\n",
       " 'avoid': 216,\n",
       " 'fast': 217,\n",
       " 'come': 218,\n",
       " 'totally': 219,\n",
       " 'low': 220,\n",
       " 'now': 221,\n",
       " 'bluetooth': 222,\n",
       " 'special': 223,\n",
       " 'tried': 224,\n",
       " 'staff': 225,\n",
       " 'everyone': 226,\n",
       " 'makes': 227,\n",
       " 'things': 228,\n",
       " 'sucks': 229,\n",
       " 'charge': 230,\n",
       " 'working': 231,\n",
       " 'actors': 232,\n",
       " 'sushi': 233,\n",
       " 'steak': 234,\n",
       " 'of': 235,\n",
       " 'customer': 236,\n",
       " 'wait': 237,\n",
       " 'clear': 238,\n",
       " 'sure': 239,\n",
       " 'fact': 240,\n",
       " 'he': 241,\n",
       " 'man': 242,\n",
       " 'least': 243,\n",
       " '3': 244,\n",
       " 'perfect': 245,\n",
       " 'felt': 246,\n",
       " 'anything': 247,\n",
       " 'interesting': 248,\n",
       " 'completely': 249,\n",
       " 'stupid': 250,\n",
       " 'kind': 251,\n",
       " 'scene': 252,\n",
       " 'almost': 253,\n",
       " 'buffet': 254,\n",
       " 'worse': 255,\n",
       " 'simply': 256,\n",
       " 'when': 257,\n",
       " 'menu': 258,\n",
       " 'actually': 259,\n",
       " 'using': 260,\n",
       " 'away': 261,\n",
       " 'beautiful': 262,\n",
       " 'item': 263,\n",
       " 'plug': 264,\n",
       " 'small': 265,\n",
       " 'taste': 266,\n",
       " 'without': 267,\n",
       " 'calls': 268,\n",
       " 'hard': 269,\n",
       " 'year': 270,\n",
       " 'seriously': 271,\n",
       " 'may': 272,\n",
       " 'device': 273,\n",
       " 'flavor': 274,\n",
       " 'gets': 275,\n",
       " 'chicken': 276,\n",
       " 'on': 277,\n",
       " 'bit': 278,\n",
       " 'incredible': 279,\n",
       " 'do': 280,\n",
       " 'especially': 281,\n",
       " 'white': 282,\n",
       " 'took': 283,\n",
       " 'them': 284,\n",
       " 'bland': 285,\n",
       " 'getting': 286,\n",
       " 'as': 287,\n",
       " 'return': 288,\n",
       " 'server': 289,\n",
       " 'coming': 290,\n",
       " 'hear': 291,\n",
       " 'care': 292,\n",
       " 'order': 293,\n",
       " 'either': 294,\n",
       " 'unfortunately': 295,\n",
       " 'call': 296,\n",
       " 'rather': 297,\n",
       " 'cell': 298,\n",
       " 'oh': 299,\n",
       " 'liked': 300,\n",
       " 'yet': 301,\n",
       " 'atmosphere': 302,\n",
       " 'different': 303,\n",
       " 'left': 304,\n",
       " 'at': 305,\n",
       " 'motorola': 306,\n",
       " 'had': 307,\n",
       " 'super': 308,\n",
       " 'received': 309,\n",
       " 'just': 310,\n",
       " 'hour': 311,\n",
       " 'an': 312,\n",
       " 'three': 313,\n",
       " 'meal': 314,\n",
       " 'writing': 315,\n",
       " 'dialogue': 316,\n",
       " 'burger': 317,\n",
       " 'deal': 318,\n",
       " 'fit': 319,\n",
       " 'art': 320,\n",
       " 'extremely': 321,\n",
       " 'barely': 322,\n",
       " 'lunch': 323,\n",
       " 'was': 324,\n",
       " 'tell': 325,\n",
       " 'disappointment': 326,\n",
       " 'prices': 327,\n",
       " 'home': 328,\n",
       " \"there's\": 329,\n",
       " 'full': 330,\n",
       " 'hands': 331,\n",
       " 'off': 332,\n",
       " 'stay': 333,\n",
       " 'light': 334,\n",
       " 'with': 335,\n",
       " 'purchase': 336,\n",
       " 'whole': 337,\n",
       " 'feeling': 338,\n",
       " 'saw': 339,\n",
       " 'none': 340,\n",
       " 'believe': 341,\n",
       " \"i'll\": 342,\n",
       " 'wrong': 343,\n",
       " 'sucked': 344,\n",
       " 'line': 345,\n",
       " 'does': 346,\n",
       " 'phones': 347,\n",
       " 'less': 348,\n",
       " 'high': 349,\n",
       " 'started': 350,\n",
       " 'volume': 351,\n",
       " 'put': 352,\n",
       " 'problems': 353,\n",
       " 'crap': 354,\n",
       " 'short': 355,\n",
       " 'disappointing': 356,\n",
       " 'boring': 357,\n",
       " 'perfectly': 358,\n",
       " 'fits': 359,\n",
       " 'days': 360,\n",
       " 'directing': 361,\n",
       " 'enjoyed': 362,\n",
       " 'bar': 363,\n",
       " 'house': 364,\n",
       " 'certainly': 365,\n",
       " 'hours': 366,\n",
       " 'side': 367,\n",
       " 'tasty': 368,\n",
       " 'effects': 369,\n",
       " 'play': 370,\n",
       " 'area': 371,\n",
       " 'amazon': 372,\n",
       " 'drama': 373,\n",
       " 'something': 374,\n",
       " 'sauce': 375,\n",
       " \"that's\": 376,\n",
       " 'breakfast': 377,\n",
       " 'expect': 378,\n",
       " 'director': 379,\n",
       " 'trying': 380,\n",
       " 'face': 381,\n",
       " 'actor': 382,\n",
       " 'hot': 383,\n",
       " 'mediocre': 384,\n",
       " 'mess': 385,\n",
       " 'playing': 386,\n",
       " 'spot': 387,\n",
       " 'looks': 388,\n",
       " 'looking': 389,\n",
       " 'family': 390,\n",
       " 'recommended': 391,\n",
       " 'need': 392,\n",
       " 'top': 393,\n",
       " 'soon': 394,\n",
       " 'enjoy': 395,\n",
       " 'seems': 396,\n",
       " 'pictures': 397,\n",
       " 'understand': 398,\n",
       " 'world': 399,\n",
       " 'did': 400,\n",
       " 'hate': 401,\n",
       " 'cannot': 402,\n",
       " 'free': 403,\n",
       " 'watched': 404,\n",
       " 'be': 405,\n",
       " 'audio': 406,\n",
       " 'fun': 407,\n",
       " 'dish': 408,\n",
       " 'junk': 409,\n",
       " 'fries': 410,\n",
       " 'company': 411,\n",
       " 'spicy': 412,\n",
       " 'unit': 413,\n",
       " 'garbage': 414,\n",
       " 'months': 415,\n",
       " 'fails': 416,\n",
       " 'voice': 417,\n",
       " 'huge': 418,\n",
       " 'priced': 419,\n",
       " 'tasted': 420,\n",
       " 'quickly': 421,\n",
       " 'making': 422,\n",
       " 'self': 423,\n",
       " 'picture': 424,\n",
       " 'dropped': 425,\n",
       " 'these': 426,\n",
       " 'couple': 427,\n",
       " 'series': 428,\n",
       " 'plus': 429,\n",
       " 'ending': 430,\n",
       " 'horror': 431,\n",
       " 'cinematography': 432,\n",
       " 'strong': 433,\n",
       " 'then': 434,\n",
       " 'together': 435,\n",
       " 'their': 436,\n",
       " 'comes': 437,\n",
       " 'truly': 438,\n",
       " 'verizon': 439,\n",
       " 'design': 440,\n",
       " 'buttons': 441,\n",
       " 'more': 442,\n",
       " 'check': 443,\n",
       " 'average': 444,\n",
       " 'original': 445,\n",
       " 'decent': 446,\n",
       " 'kept': 447,\n",
       " 'later': 448,\n",
       " 'chips': 449,\n",
       " 'waitress': 450,\n",
       " 'glad': 451,\n",
       " 'weak': 452,\n",
       " 'inside': 453,\n",
       " 'management': 454,\n",
       " 'will': 455,\n",
       " 'pleased': 456,\n",
       " 'authentic': 457,\n",
       " 'else': 458,\n",
       " 'played': 459,\n",
       " 'wall': 460,\n",
       " 'headsets': 461,\n",
       " 'dishes': 462,\n",
       " 'warm': 463,\n",
       " 'color': 464,\n",
       " 'useless': 465,\n",
       " 'predictable': 466,\n",
       " 'broke': 467,\n",
       " 'table': 468,\n",
       " 'overpriced': 469,\n",
       " 'possible': 470,\n",
       " 'down': 471,\n",
       " 'both': 472,\n",
       " 'brilliant': 473,\n",
       " 'easily': 474,\n",
       " 'feels': 475,\n",
       " 'pay': 476,\n",
       " 'word': 477,\n",
       " 'literally': 478,\n",
       " 'course': 479,\n",
       " 'software': 480,\n",
       " 'internet': 481,\n",
       " 'tv': 482,\n",
       " 'subtle': 483,\n",
       " 'given': 484,\n",
       " '4': 485,\n",
       " 'set': 486,\n",
       " 'wasted': 487,\n",
       " 'wanted': 488,\n",
       " 'human': 489,\n",
       " 'solid': 490,\n",
       " 'guess': 491,\n",
       " 'expected': 492,\n",
       " 'value': 493,\n",
       " 'cold': 494,\n",
       " 'jabra': 495,\n",
       " 'simple': 496,\n",
       " 'ago': 497,\n",
       " 'star': 498,\n",
       " 'keep': 499,\n",
       " '20': 500,\n",
       " 'half': 501,\n",
       " 'rude': 502,\n",
       " 'kids': 503,\n",
       " 'others': 504,\n",
       " 'rent': 505,\n",
       " 'today': 506,\n",
       " 'second': 507,\n",
       " 'sick': 508,\n",
       " 'range': 509,\n",
       " 'living': 510,\n",
       " 'annoying': 511,\n",
       " 'waited': 512,\n",
       " 'exactly': 513,\n",
       " 'within': 514,\n",
       " 'longer': 515,\n",
       " 'happened': 516,\n",
       " 'store': 517,\n",
       " 'town': 518,\n",
       " 'lacking': 519,\n",
       " 'budget': 520,\n",
       " 'ambiance': 521,\n",
       " 'tender': 522,\n",
       " 'nokia': 523,\n",
       " 'mistake': 524,\n",
       " 'its': 525,\n",
       " 'instead': 526,\n",
       " 'places': 527,\n",
       " 'selection': 528,\n",
       " 'helpful': 529,\n",
       " 'his': 530,\n",
       " 'running': 531,\n",
       " 'samsung': 532,\n",
       " 'bother': 533,\n",
       " 'mind': 534,\n",
       " 'holes': 535,\n",
       " 'editing': 536,\n",
       " 'meat': 537,\n",
       " 'seemed': 538,\n",
       " 'close': 539,\n",
       " 'finally': 540,\n",
       " 'bars': 541,\n",
       " 'wear': 542,\n",
       " 'location': 543,\n",
       " 'attentive': 544,\n",
       " 'lines': 545,\n",
       " 'memorable': 546,\n",
       " 'ears': 547,\n",
       " 'mention': 548,\n",
       " 'superb': 549,\n",
       " 'quick': 550,\n",
       " 'although': 551,\n",
       " 'entire': 552,\n",
       " 'reasonable': 553,\n",
       " 'let': 554,\n",
       " 'mostly': 555,\n",
       " 'arrived': 556,\n",
       " 'pho': 557,\n",
       " 'single': 558,\n",
       " 'cinema': 559,\n",
       " 'obviously': 560,\n",
       " 'damn': 561,\n",
       " 'sometimes': 562,\n",
       " 'needed': 563,\n",
       " 'amount': 564,\n",
       " 'beyond': 565,\n",
       " '8': 566,\n",
       " 'from': 567,\n",
       " 'our': 568,\n",
       " 'loud': 569,\n",
       " 'beer': 570,\n",
       " 'hit': 571,\n",
       " 'sandwich': 572,\n",
       " 'while': 573,\n",
       " 'goes': 574,\n",
       " 'gave': 575,\n",
       " 'only': 576,\n",
       " 'friends': 577,\n",
       " 'over': 578,\n",
       " 'features': 579,\n",
       " 'told': 580,\n",
       " 'ridiculous': 581,\n",
       " 'zero': 582,\n",
       " 'twice': 583,\n",
       " 'forever': 584,\n",
       " 'bring': 585,\n",
       " 'mobile': 586,\n",
       " 'have': 587,\n",
       " 'which': 588,\n",
       " 'review': 589,\n",
       " 'perhaps': 590,\n",
       " 'casting': 591,\n",
       " 'period': 592,\n",
       " 'seafood': 593,\n",
       " 'room': 594,\n",
       " 'fish': 595,\n",
       " 'reviews': 596,\n",
       " 'stuff': 597,\n",
       " 'trip': 598,\n",
       " 'important': 599,\n",
       " 'feature': 600,\n",
       " 'joy': 601,\n",
       " 'mean': 602,\n",
       " 'pathetic': 603,\n",
       " 'computer': 604,\n",
       " 'unbelievable': 605,\n",
       " 'girl': 606,\n",
       " 'portrayal': 607,\n",
       " 'plastic': 608,\n",
       " 'front': 609,\n",
       " 'portions': 610,\n",
       " 'extra': 611,\n",
       " 'person': 612,\n",
       " 'thin': 613,\n",
       " 'ice': 614,\n",
       " 'satisfied': 615,\n",
       " 'cable': 616,\n",
       " 'dry': 617,\n",
       " 'bread': 618,\n",
       " 'cooked': 619,\n",
       " 'potato': 620,\n",
       " 'headphones': 621,\n",
       " 'comedy': 622,\n",
       " 'charm': 623,\n",
       " 'outside': 624,\n",
       " 'break': 625,\n",
       " 'reason': 626,\n",
       " 'dead': 627,\n",
       " 'eating': 628,\n",
       " 'unless': 629,\n",
       " '\\x96': 630,\n",
       " 'cases': 631,\n",
       " 'lost': 632,\n",
       " 'thumbs': 633,\n",
       " 'etc': 634,\n",
       " 'direction': 635,\n",
       " 'costs': 636,\n",
       " 'week': 637,\n",
       " 'poorly': 638,\n",
       " 'involved': 639,\n",
       " 'seem': 640,\n",
       " 'hope': 641,\n",
       " 'style': 642,\n",
       " 'roles': 643,\n",
       " 'book': 644,\n",
       " 'who': 645,\n",
       " 'maybe': 646,\n",
       " 'written': 647,\n",
       " 'usual': 648,\n",
       " 'replace': 649,\n",
       " 'non': 650,\n",
       " 'lead': 651,\n",
       " 'water': 652,\n",
       " 'elsewhere': 653,\n",
       " 'were': 654,\n",
       " 'once': 655,\n",
       " '7': 656,\n",
       " 'dining': 657,\n",
       " 'bargain': 658,\n",
       " 'clean': 659,\n",
       " 'several': 660,\n",
       " 'cant': 661,\n",
       " 'sub': 662,\n",
       " 'dirty': 663,\n",
       " 'sad': 664,\n",
       " 'fans': 665,\n",
       " 'whatever': 666,\n",
       " 'flick': 667,\n",
       " 'par': 668,\n",
       " 'impressive': 669,\n",
       " 'recent': 670,\n",
       " 'particular': 671,\n",
       " 'suspense': 672,\n",
       " 'plain': 673,\n",
       " 'drive': 674,\n",
       " 'throughout': 675,\n",
       " 'basically': 676,\n",
       " 'touch': 677,\n",
       " 'walked': 678,\n",
       " 'razr': 679,\n",
       " 'due': 680,\n",
       " 'served': 681,\n",
       " 'difficult': 682,\n",
       " 'rare': 683,\n",
       " 'someone': 684,\n",
       " 'leave': 685,\n",
       " 'waiting': 686,\n",
       " 'pasta': 687,\n",
       " 'hand': 688,\n",
       " 'classic': 689,\n",
       " 'trash': 690,\n",
       " 'heart': 691,\n",
       " 'large': 692,\n",
       " '30': 693,\n",
       " 'become': 694,\n",
       " 'cover': 695,\n",
       " 'dinner': 696,\n",
       " 'production': 697,\n",
       " 'shrimp': 698,\n",
       " 'turn': 699,\n",
       " 'fan': 700,\n",
       " 'data': 701,\n",
       " 'player': 702,\n",
       " 'clever': 703,\n",
       " \"doesn't\": 704,\n",
       " 'hold': 705,\n",
       " 'lacks': 706,\n",
       " 'hilarious': 707,\n",
       " 'party': 708,\n",
       " 'lacked': 709,\n",
       " 'adorable': 710,\n",
       " 'action': 711,\n",
       " 'visit': 712,\n",
       " 'asked': 713,\n",
       " 'passed': 714,\n",
       " 'drago': 715,\n",
       " 'blue': 716,\n",
       " 'strip': 717,\n",
       " 'needs': 718,\n",
       " 'wine': 719,\n",
       " 'immediately': 720,\n",
       " 'able': 721,\n",
       " 'issues': 722,\n",
       " 'incredibly': 723,\n",
       " 'im': 724,\n",
       " 'rating': 725,\n",
       " 'note': 726,\n",
       " 'vibe': 727,\n",
       " 'created': 728,\n",
       " 'him': 729,\n",
       " 'ability': 730,\n",
       " 'sweet': 731,\n",
       " 'keyboard': 732,\n",
       " 'storyline': 733,\n",
       " 'tool': 734,\n",
       " 'live': 735,\n",
       " 'appreciate': 736,\n",
       " 'towards': 737,\n",
       " 'drinks': 738,\n",
       " 'stop': 739,\n",
       " 'video': 740,\n",
       " 'included': 741,\n",
       " 'bored': 742,\n",
       " 'cut': 743,\n",
       " 'clearly': 744,\n",
       " 'decision': 745,\n",
       " 'how': 746,\n",
       " 'thai': 747,\n",
       " 'business': 748,\n",
       " \"we'll\": 749,\n",
       " 'wings': 750,\n",
       " 'cream': 751,\n",
       " 'dessert': 752,\n",
       " 'young': 753,\n",
       " 'usb': 754,\n",
       " 'missed': 755,\n",
       " 'please': 756,\n",
       " 'phoenix': 757,\n",
       " 'run': 758,\n",
       " 'games': 759,\n",
       " 'idea': 760,\n",
       " 'pair': 761,\n",
       " 'thoroughly': 762,\n",
       " 'dont': 763,\n",
       " 'loves': 764,\n",
       " 'weird': 765,\n",
       " 'along': 766,\n",
       " 'starts': 767,\n",
       " 'thinking': 768,\n",
       " 'some': 769,\n",
       " 'owner': 770,\n",
       " 'advise': 771,\n",
       " 'wasting': 772,\n",
       " 'rice': 773,\n",
       " 'turns': 774,\n",
       " 'pleasant': 775,\n",
       " 'true': 776,\n",
       " 'fall': 777,\n",
       " 'husband': 778,\n",
       " 'conclusion': 779,\n",
       " 'total': 780,\n",
       " 'reasonably': 781,\n",
       " 'recently': 782,\n",
       " 'visual': 783,\n",
       " 'ups': 784,\n",
       " 'seller': 785,\n",
       " 'considering': 786,\n",
       " 'level': 787,\n",
       " 'torture': 788,\n",
       " 'called': 789,\n",
       " 'wow': 790,\n",
       " 'your': 791,\n",
       " 'attempt': 792,\n",
       " 'sturdy': 793,\n",
       " 'owners': 794,\n",
       " 'list': 795,\n",
       " 'generally': 796,\n",
       " 'dark': 797,\n",
       " 'takes': 798,\n",
       " 'despite': 799,\n",
       " 'exceptional': 800,\n",
       " 'rate': 801,\n",
       " 'expensive': 802,\n",
       " 'ringtones': 803,\n",
       " 'form': 804,\n",
       " 'connection': 805,\n",
       " 'shipping': 806,\n",
       " 'she': 807,\n",
       " 'five': 808,\n",
       " 'paid': 809,\n",
       " 'manager': 810,\n",
       " 'minute': 811,\n",
       " 'parts': 812,\n",
       " 'killer': 813,\n",
       " 'charged': 814,\n",
       " 'stories': 815,\n",
       " 'station': 816,\n",
       " 'vegetables': 817,\n",
       " 'showed': 818,\n",
       " 'disgusting': 819,\n",
       " 'guy': 820,\n",
       " 'shots': 821,\n",
       " 'shot': 822,\n",
       " 'red': 823,\n",
       " 'before': 824,\n",
       " 'lots': 825,\n",
       " 'believable': 826,\n",
       " 'thriller': 827,\n",
       " 'performances': 828,\n",
       " 'lame': 829,\n",
       " 'death': 830,\n",
       " 'previous': 831,\n",
       " 'point': 832,\n",
       " 'ask': 833,\n",
       " 'seeing': 834,\n",
       " 'q': 835,\n",
       " 'waiter': 836,\n",
       " '12': 837,\n",
       " 'offers': 838,\n",
       " 'power': 839,\n",
       " 'type': 840,\n",
       " 'honest': 841,\n",
       " 'indeed': 842,\n",
       " 'bacon': 843,\n",
       " 'charging': 844,\n",
       " 'audience': 845,\n",
       " 'deserves': 846,\n",
       " 'insult': 847,\n",
       " 'eaten': 848,\n",
       " 'looked': 849,\n",
       " 'unreliable': 850,\n",
       " 'likes': 851,\n",
       " 'ended': 852,\n",
       " 'directed': 853,\n",
       " 'soup': 854,\n",
       " 'lightweight': 855,\n",
       " 'thats': 856,\n",
       " 'leather': 857,\n",
       " 'favorite': 858,\n",
       " 'mic': 859,\n",
       " 'cingular': 860,\n",
       " 't': 861,\n",
       " 'chinese': 862,\n",
       " 'imagination': 863,\n",
       " 'tasteless': 864,\n",
       " 'purchased': 865,\n",
       " 'owned': 866,\n",
       " 'tom': 867,\n",
       " 'hair': 868,\n",
       " 'turned': 869,\n",
       " 'silent': 870,\n",
       " 'anytime': 871,\n",
       " 'premise': 872,\n",
       " 'holds': 873,\n",
       " 'age': 874,\n",
       " 'john': 875,\n",
       " 'ray': 876,\n",
       " 'terrific': 877,\n",
       " 'fear': 878,\n",
       " 'ok': 879,\n",
       " 'checked': 880,\n",
       " 'tea': 881,\n",
       " 'italian': 882,\n",
       " 'theater': 883,\n",
       " 'comfortably': 884,\n",
       " 'cute': 885,\n",
       " 'occasionally': 886,\n",
       " 'mexican': 887,\n",
       " 'forget': 888,\n",
       " 'cult': 889,\n",
       " 'buying': 890,\n",
       " 'might': 891,\n",
       " 'mouth': 892,\n",
       " 'setting': 893,\n",
       " 'palm': 894,\n",
       " 'signal': 895,\n",
       " 'moving': 896,\n",
       " 'tacos': 897,\n",
       " 'worthless': 898,\n",
       " 'crisp': 899,\n",
       " 'anyway': 900,\n",
       " 'history': 901,\n",
       " 'flat': 902,\n",
       " 'intelligence': 903,\n",
       " 'rest': 904,\n",
       " 'beef': 905,\n",
       " 'about': 906,\n",
       " 'hitchcock': 907,\n",
       " 'songs': 908,\n",
       " 'gotten': 909,\n",
       " 'scamp': 910,\n",
       " 'pretentious': 911,\n",
       " 'ate': 912,\n",
       " 'salmon': 913,\n",
       " 'pros': 914,\n",
       " 'hell': 915,\n",
       " 'giving': 916,\n",
       " 'duck': 917,\n",
       " 'generous': 918,\n",
       " 'failed': 919,\n",
       " 'plugged': 920,\n",
       " 'sides': 921,\n",
       " 'desserts': 922,\n",
       " 'surprised': 923,\n",
       " 'mother': 924,\n",
       " 'pull': 925,\n",
       " 'her': 926,\n",
       " 'pizzas': 927,\n",
       " 'nearly': 928,\n",
       " 'pitiful': 929,\n",
       " 'means': 930,\n",
       " 'number': 931,\n",
       " 'yeah': 932,\n",
       " 'dance': 933,\n",
       " 'industry': 934,\n",
       " 'whatsoever': 935,\n",
       " 'putting': 936,\n",
       " 'greek': 937,\n",
       " 'dressing': 938,\n",
       " 'pita': 939,\n",
       " 'hummus': 940,\n",
       " 'speaker': 941,\n",
       " 'size': 942,\n",
       " 'consider': 943,\n",
       " 'description': 944,\n",
       " 'date': 945,\n",
       " 'main': 946,\n",
       " 'intelligent': 947,\n",
       " 'wise': 948,\n",
       " 'learn': 949,\n",
       " 'emotions': 950,\n",
       " 'song': 951,\n",
       " 'explain': 952,\n",
       " 'martin': 953,\n",
       " 'chemistry': 954,\n",
       " 'dancing': 955,\n",
       " 'regular': 956,\n",
       " 'begin': 957,\n",
       " 'construction': 958,\n",
       " 'myself': 959,\n",
       " 'drink': 960,\n",
       " 'final': 961,\n",
       " 'whether': 962,\n",
       " 'pocket': 963,\n",
       " 'brought': 964,\n",
       " 'chef': 965,\n",
       " 'sharp': 966,\n",
       " 'deeply': 967,\n",
       " 'machine': 968,\n",
       " 'decor': 969,\n",
       " 'soundtrack': 970,\n",
       " 'gone': 971,\n",
       " 'nut': 972,\n",
       " 'sea': 973,\n",
       " 'faux': 974,\n",
       " 'apart': 975,\n",
       " 'baby': 976,\n",
       " 'early': 977,\n",
       " 'unconvincing': 978,\n",
       " 'realized': 979,\n",
       " 'sunglasses': 980,\n",
       " 'nonsense': 981,\n",
       " 'breaks': 982,\n",
       " 'clip': 983,\n",
       " '35': 984,\n",
       " 'seconds': 985,\n",
       " 'user': 986,\n",
       " 'ease': 987,\n",
       " 'heard': 988,\n",
       " 'salt': 989,\n",
       " 'chewy': 990,\n",
       " 'pork': 991,\n",
       " 'pack': 992,\n",
       " 'drop': 993,\n",
       " 'utterly': 994,\n",
       " 'flaws': 995,\n",
       " 'presents': 996,\n",
       " 'wonder': 997,\n",
       " 'happen': 998,\n",
       " 'behind': 999,\n",
       " 'tale': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 210, 1, 1], [3, 361, 396, 1]]\n"
     ]
    }
   ],
   "source": [
    "dataset_train_sequences = tokenizer.texts_to_sequences(dataset_train.loc[:,'sentence'])\n",
    "dataset_test_sequences = tokenizer.texts_to_sequences(dataset_test.loc[:,'sentence'])\n",
    "print(dataset_train_sequences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O último passo de pré-processamento agora consiste em realizar o padding das sequências.\n",
    "\n",
    "Para isso, utilizaremos a funcão [`pad_sequences`](https://keras.io/preprocessing/sequence/)\n",
    "\n",
    "Os principais argumentos dessa funcão são:\n",
    "\n",
    "- `maxlen`: tamanho da sequência a ser gerada.\n",
    "- `padding`: 'pre' para adicionar zeros à esquerda e 'post' para adicionar zeros à direita.\n",
    "- `truncating`: 'pre' para remover palavras no comeco da frase se for maior que o tamanho especificado, 'post' para remover do final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "[[  1 210   1 ...   0   0   0]\n",
      " [  3 361 396 ...   0   0   0]\n",
      " [  2   1   1 ...   0   0   0]\n",
      " ...\n",
      " [  2 107 186 ...   0   0   0]\n",
      " [ 49 492   0 ...   0   0   0]\n",
      " [  1  19  10 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500  #Tamanho máximo da frase\n",
    "padding_type = 'post'\n",
    "truncating_type = 'post'\n",
    "\n",
    "dataset_train_sequences = pad_sequences(dataset_train_sequences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "dataset_test_sequences = pad_sequences(dataset_test_sequences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "\n",
    "print(len(dataset_train_sequences[0]))\n",
    "print(len(dataset_train_sequences[1]))\n",
    "print(dataset_train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que as sentencas estão em um formato favorável, podemos treinar nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicione a sua arquitetura, lembrando que a entrada tem tamanho maxlen e a saída 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(512, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 500, 16)           8192      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               1024128   \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,034,017\n",
      "Trainable params: 1,034,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defina aqui seu otimizador e sua loss\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335\n",
      "2335\n",
      "413\n",
      "413\n",
      "Train on 2335 samples, validate on 413 samples\n",
      "Epoch 1/20\n",
      "2335/2335 - 4s - loss: 0.6936 - accuracy: 0.5255 - val_loss: 0.6897 - val_accuracy: 0.6271\n",
      "Epoch 2/20\n",
      "2335/2335 - 3s - loss: 0.6122 - accuracy: 0.6809 - val_loss: 0.5367 - val_accuracy: 0.7288\n",
      "Epoch 3/20\n",
      "2335/2335 - 3s - loss: 0.4241 - accuracy: 0.8051 - val_loss: 0.5874 - val_accuracy: 0.7240\n",
      "Epoch 4/20\n",
      "2335/2335 - 3s - loss: 0.3575 - accuracy: 0.8385 - val_loss: 0.5822 - val_accuracy: 0.7312\n",
      "Epoch 5/20\n",
      "2335/2335 - 3s - loss: 0.3319 - accuracy: 0.8505 - val_loss: 0.5050 - val_accuracy: 0.7724\n",
      "Epoch 6/20\n",
      "2335/2335 - 3s - loss: 0.3046 - accuracy: 0.8587 - val_loss: 0.5214 - val_accuracy: 0.7603\n",
      "Epoch 7/20\n",
      "2335/2335 - 3s - loss: 0.2840 - accuracy: 0.8754 - val_loss: 0.6331 - val_accuracy: 0.7385\n",
      "Epoch 8/20\n",
      "2335/2335 - 3s - loss: 0.2679 - accuracy: 0.8835 - val_loss: 0.6149 - val_accuracy: 0.7506\n",
      "Epoch 9/20\n",
      "2335/2335 - 3s - loss: 0.2537 - accuracy: 0.8887 - val_loss: 0.6123 - val_accuracy: 0.7482\n",
      "Epoch 10/20\n",
      "2335/2335 - 3s - loss: 0.2377 - accuracy: 0.8946 - val_loss: 0.7263 - val_accuracy: 0.7433\n",
      "Epoch 11/20\n",
      "2335/2335 - 3s - loss: 0.2139 - accuracy: 0.9054 - val_loss: 0.7297 - val_accuracy: 0.7337\n",
      "Epoch 12/20\n",
      "2335/2335 - 3s - loss: 0.1946 - accuracy: 0.9173 - val_loss: 0.8750 - val_accuracy: 0.7240\n",
      "Epoch 13/20\n",
      "2335/2335 - 3s - loss: 0.1739 - accuracy: 0.9306 - val_loss: 0.8462 - val_accuracy: 0.7240\n",
      "Epoch 14/20\n",
      "2335/2335 - 3s - loss: 0.1537 - accuracy: 0.9379 - val_loss: 0.9819 - val_accuracy: 0.7143\n",
      "Epoch 15/20\n",
      "2335/2335 - 3s - loss: 0.1354 - accuracy: 0.9478 - val_loss: 0.9787 - val_accuracy: 0.7046\n",
      "Epoch 16/20\n",
      "2335/2335 - 3s - loss: 0.1194 - accuracy: 0.9490 - val_loss: 1.0837 - val_accuracy: 0.7070\n",
      "Epoch 17/20\n",
      "2335/2335 - 3s - loss: 0.1046 - accuracy: 0.9589 - val_loss: 1.1464 - val_accuracy: 0.6973\n",
      "Epoch 18/20\n",
      "2335/2335 - 3s - loss: 0.0932 - accuracy: 0.9606 - val_loss: 1.2367 - val_accuracy: 0.7119\n",
      "Epoch 19/20\n",
      "2335/2335 - 3s - loss: 0.0842 - accuracy: 0.9666 - val_loss: 1.3598 - val_accuracy: 0.6973\n",
      "Epoch 20/20\n",
      "2335/2335 - 3s - loss: 0.0765 - accuracy: 0.9700 - val_loss: 1.4878 - val_accuracy: 0.7046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2164d72b208>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "num_epochs = 20\n",
    "\n",
    "train_seqs = dataset_train_sequences\n",
    "train_labels = np.array(dataset_train.loc[:, 'label'])\n",
    "test_seqs = dataset_test_sequences\n",
    "test_labels = np.array(dataset_test.loc[:, 'label'])\n",
    "\n",
    "print(len(train_seqs))\n",
    "print(len(train_labels))\n",
    "print(len(test_seqs))\n",
    "print(len(test_labels))\n",
    "\n",
    "model.fit(train_seqs,train_labels, epochs = num_epochs, validation_data=(test_seqs,test_labels), verbose=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar se as classificacões fazem sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 43   5   8 ...   0   0   0]\n",
      " [ 82 266   0 ...   0   0   0]\n",
      " [ 68  30  24 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "or_test_sentences = ['very good movie', 'terrible taste', 'worst product ever']\n",
    "#codificando\n",
    "test_sentences = tokenizer.texts_to_sequences(or_test_sentences)\n",
    "test_sentences = pad_sequences(test_sentences, maxlen = maxlen, padding=padding_type, truncating=truncating_type)\n",
    "\n",
    "print(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very good movie', 'terrible taste', 'worst product ever']\n",
      "[[ True]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences)\n",
    "print(or_test_sentences)\n",
    "print(predictions > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie como o número de dimensões do embedding, o tipo do padding, o tamanho do vocabulário, o tamanho máximo de sentenca, etc. contribuem para a qualidade do modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
